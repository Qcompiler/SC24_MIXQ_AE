srun: error: ioctl(TIOCGWINSZ): Inappropriate ioctl for device
srun: error: Not using a pseudo-terminal, disregarding --pty option
srun: error: ioctl(TIOCGWINSZ): Inappropriate ioctl for device
srun: error: Not using a pseudo-terminal, disregarding --pty option
srun: error: ioctl(TIOCGWINSZ): Inappropriate ioctl for device
srun: error: Not using a pseudo-terminal, disregarding --pty option
srun: error: ioctl(TIOCGWINSZ): Inappropriate ioctl for device
srun: error: Not using a pseudo-terminal, disregarding --pty option
/home/dataset/quant/quant8/down_weight_only/Llama-2-7b
use oneline quant methods
LlamaConfig {
  "_name_or_path": "/home/dataset/Llama-2-7b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_new_tokens": 4096,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 32000
}

/home/dataset/quant/quant4/down_weight_only/Llama-2-13b
use oneline quant methods
LlamaConfig {
  "_name_or_path": "/home/dataset/Llama-2-13b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_new_tokens": 4096,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 32000
}

/home/dataset/quant/quant4/down_weight_only/Llama-2-7b
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]use oneline quant methods
LlamaConfig {
  "_name_or_path": "/home/dataset/Llama-2-7b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_new_tokens": 4096,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/dataset/quant/quant8/down_weight_only/Llama-2-13b
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]use oneline quant methods
LlamaConfig {
  "_name_or_path": "/home/dataset/Llama-2-13b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_new_tokens": 4096,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:00,  1.06it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.11s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.97it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.78it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.24it/s]
LlamaMixQForCausalLM(
LlamaMixQForCausalLM(
  (model): LlamaForCausalLM(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 5120)
      (embed_tokens): Embedding(32000, 5120)
      (layers): ModuleList(
      (layers): ModuleList(
        (0-39): 40 x LlamaDecoderLayer(
        (0-39): 40 x LlamaDecoderLayer(
          (self_attn): LlamaSdpaAttention(
          (self_attn): LlamaSdpaAttention(
            (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
            (rotary_emb): LlamaRotaryEmbedding()
          )
          )
          (mlp): LlamaMLP(
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
            (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
            (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
            (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
            (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
            (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
            (act_fn): SiLU()
            (act_fn): SiLU()
          )
          )
          (input_layernorm): LlamaRMSNorm()
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=5120, out_features=32000, bias=False)
  )
)
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=5120, out_features=32000, bias=False)
  )
)
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
LlamaMixQForCausalLM(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 4096)
      (layers): ModuleList(
        (0-31): 32 x LlamaDecoderLayer(
          (self_attn): LlamaSdpaAttention(
            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
  )
)
LlamaMixQForCausalLM(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 4096)
      (layers): ModuleList(
        (0-31): 32 x LlamaDecoderLayer(
          (self_attn): LlamaSdpaAttention(
            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
  )
)
Mix quant:   0%|          | 0/32 [00:00<?, ?it/s]Mix quant:   0%|          | 0/32 [00:00<?, ?it/s]Mix quant:   3%|▎         | 1/32 [00:02<01:07,  2.18s/it]Mix quant:   3%|▎         | 1/32 [00:03<01:33,  3.01s/it]Mix quant:   0%|          | 0/40 [00:00<?, ?it/s]Mix quant:   0%|          | 0/40 [00:00<?, ?it/s]Mix quant:   2%|▎         | 1/40 [00:02<01:30,  2.32s/it]Mix quant:   2%|▎         | 1/40 [00:03<02:25,  3.72s/it]Mix quant:   6%|▋         | 2/32 [00:08<02:19,  4.66s/it]Mix quant:   6%|▋         | 2/32 [00:09<02:27,  4.91s/it]Mix quant:   5%|▌         | 2/40 [00:08<02:57,  4.66s/it]Mix quant:   5%|▌         | 2/40 [00:09<03:16,  5.16s/it]Mix quant:   9%|▉         | 3/32 [00:14<02:37,  5.42s/it]Mix quant:   9%|▉         | 3/32 [00:15<02:41,  5.57s/it]Mix quant:   8%|▊         | 3/40 [00:13<03:03,  4.96s/it]Mix quant:   8%|▊         | 3/40 [00:15<03:13,  5.22s/it]Mix quant:  12%|█▎        | 4/32 [00:21<02:42,  5.81s/it]Mix quant:  12%|█▎        | 4/32 [00:22<02:45,  5.91s/it]Mix quant:  10%|█         | 4/40 [00:21<03:42,  6.17s/it]Mix quant:  16%|█▌        | 5/32 [00:27<02:44,  6.10s/it]Mix quant:  10%|█         | 4/40 [00:23<03:48,  6.34s/it]Mix quant:  16%|█▌        | 5/32 [00:28<02:45,  6.14s/it]Mix quant:  19%|█▉        | 6/32 [00:35<02:49,  6.54s/it]Mix quant:  12%|█▎        | 5/40 [00:30<04:10,  7.15s/it]Mix quant:  19%|█▉        | 6/32 [00:36<02:51,  6.58s/it]Mix quant:  12%|█▎        | 5/40 [00:32<04:12,  7.22s/it]Mix quant:  15%|█▌        | 6/40 [00:37<03:52,  6.83s/it]Mix quant:  22%|██▏       | 7/32 [00:42<02:45,  6.60s/it]Mix quant:  22%|██▏       | 7/32 [00:42<02:45,  6.63s/it]Mix quant:  15%|█▌        | 6/40 [00:38<03:54,  6.89s/it]Mix quant:  18%|█▊        | 7/40 [00:43<03:36,  6.55s/it]Mix quant:  25%|██▌       | 8/32 [00:47<02:32,  6.36s/it]Mix quant:  25%|██▌       | 8/32 [00:48<02:32,  6.37s/it]Mix quant:  18%|█▊        | 7/40 [00:44<03:38,  6.61s/it]Mix quant:  28%|██▊       | 9/32 [00:51<02:07,  5.52s/it]Mix quant:  20%|██        | 8/40 [00:47<03:06,  5.82s/it]Mix quant:  28%|██▊       | 9/32 [00:52<02:07,  5.54s/it]Mix quant:  20%|██        | 8/40 [00:48<03:06,  5.84s/it]Mix quant:  31%|███▏      | 10/32 [00:55<01:48,  4.94s/it]Mix quant:  31%|███▏      | 10/32 [00:55<01:48,  4.95s/it]Mix quant:  22%|██▎       | 9/40 [00:51<02:46,  5.38s/it]Mix quant:  22%|██▎       | 9/40 [00:52<02:47,  5.40s/it]Mix quant:  34%|███▍      | 11/32 [00:59<01:39,  4.75s/it]Mix quant:  34%|███▍      | 11/32 [01:00<01:40,  4.77s/it]Mix quant:  25%|██▌       | 10/40 [00:56<02:35,  5.19s/it]Mix quant:  25%|██▌       | 10/40 [00:57<02:36,  5.23s/it]Mix quant:  38%|███▊      | 12/32 [01:03<01:33,  4.67s/it]Mix quant:  38%|███▊      | 12/32 [01:04<01:33,  4.66s/it]Mix quant:  41%|████      | 13/32 [01:09<01:31,  4.80s/it]Mix quant:  41%|████      | 13/32 [01:09<01:31,  4.80s/it]Mix quant:  28%|██▊       | 11/40 [01:05<03:07,  6.47s/it]Mix quant:  28%|██▊       | 11/40 [01:07<03:07,  6.46s/it]Mix quant:  44%|████▍     | 14/32 [01:14<01:31,  5.07s/it]Mix quant:  44%|████▍     | 14/32 [01:15<01:31,  5.07s/it]Mix quant:  30%|███       | 12/40 [01:11<02:55,  6.28s/it]Mix quant:  30%|███       | 12/40 [01:12<02:55,  6.27s/it]Mix quant:  47%|████▋     | 15/32 [01:19<01:24,  4.98s/it]Mix quant:  47%|████▋     | 15/32 [01:20<01:24,  4.98s/it]Mix quant:  50%|█████     | 16/32 [01:25<01:25,  5.35s/it]Mix quant:  32%|███▎      | 13/40 [01:21<03:18,  7.36s/it]Mix quant:  50%|█████     | 16/32 [01:26<01:25,  5.34s/it]Mix quant:  32%|███▎      | 13/40 [01:22<03:19,  7.38s/it]Mix quant:  53%|█████▎    | 17/32 [01:32<01:24,  5.65s/it]Mix quant:  53%|█████▎    | 17/32 [01:32<01:24,  5.65s/it]Mix quant:  35%|███▌      | 14/40 [01:32<03:37,  8.38s/it]Mix quant:  56%|█████▋    | 18/32 [01:37<01:18,  5.58s/it]Mix quant:  35%|███▌      | 14/40 [01:33<03:38,  8.39s/it]Mix quant:  56%|█████▋    | 18/32 [01:38<01:18,  5.60s/it]Mix quant:  59%|█████▉    | 19/32 [01:43<01:13,  5.62s/it]Mix quant:  59%|█████▉    | 19/32 [01:43<01:13,  5.62s/it]Mix quant:  38%|███▊      | 15/40 [01:39<03:18,  7.95s/it]Mix quant:  38%|███▊      | 15/40 [01:40<03:18,  7.92s/it]Mix quant:  62%|██████▎   | 20/32 [01:47<01:02,  5.25s/it]Mix quant:  62%|██████▎   | 20/32 [01:48<01:03,  5.27s/it]Mix quant:  40%|████      | 16/40 [01:45<02:55,  7.32s/it]Mix quant:  40%|████      | 16/40 [01:46<02:55,  7.32s/it]Mix quant:  66%|██████▌   | 21/32 [01:52<00:57,  5.27s/it]Mix quant:  66%|██████▌   | 21/32 [01:53<00:58,  5.28s/it]Mix quant:  42%|████▎     | 17/40 [01:50<02:32,  6.64s/it]Mix quant:  42%|████▎     | 17/40 [01:51<02:33,  6.67s/it]Mix quant:  69%|██████▉   | 22/32 [01:58<00:52,  5.26s/it]Mix quant:  69%|██████▉   | 22/32 [01:58<00:52,  5.24s/it]Mix quant:  45%|████▌     | 18/40 [01:56<02:26,  6.67s/it]Mix quant:  72%|███████▏  | 23/32 [02:02<00:44,  4.92s/it]Mix quant:  45%|████▌     | 18/40 [01:58<02:27,  6.69s/it]Mix quant:  72%|███████▏  | 23/32 [02:03<00:44,  4.92s/it]Mix quant:  48%|████▊     | 19/40 [02:02<02:12,  6.32s/it]Mix quant:  75%|███████▌  | 24/32 [02:07<00:40,  5.02s/it]Mix quant:  48%|████▊     | 19/40 [02:03<02:12,  6.32s/it]Mix quant:  75%|███████▌  | 24/32 [02:08<00:40,  5.05s/it]Mix quant:  78%|███████▊  | 25/32 [02:13<00:36,  5.23s/it]Mix quant:  78%|███████▊  | 25/32 [02:14<00:36,  5.22s/it]Mix quant:  50%|█████     | 20/40 [02:10<02:15,  6.76s/it]Mix quant:  50%|█████     | 20/40 [02:11<02:15,  6.76s/it]Mix quant:  81%|████████▏ | 26/32 [02:18<00:31,  5.26s/it]Mix quant:  81%|████████▏ | 26/32 [02:19<00:31,  5.24s/it]Mix quant:  84%|████████▍ | 27/32 [02:24<00:26,  5.34s/it]Mix quant:  52%|█████▎    | 21/40 [02:19<02:24,  7.59s/it]Mix quant:  84%|████████▍ | 27/32 [02:24<00:26,  5.32s/it]Mix quant:  52%|█████▎    | 21/40 [02:20<02:24,  7.58s/it]Mix quant:  88%|████████▊ | 28/32 [02:31<00:23,  5.86s/it]Mix quant:  88%|████████▊ | 28/32 [02:31<00:23,  5.86s/it]Mix quant:  55%|█████▌    | 22/40 [02:27<02:20,  7.78s/it]Mix quant:  55%|█████▌    | 22/40 [02:29<02:19,  7.77s/it]Mix quant:  91%|█████████ | 29/32 [02:37<00:17,  5.86s/it]Mix quant:  91%|█████████ | 29/32 [02:37<00:17,  5.85s/it]Mix quant:  57%|█████▊    | 23/40 [02:36<02:16,  8.01s/it]Mix quant:  94%|█████████▍| 30/32 [02:41<00:10,  5.46s/it]Mix quant:  94%|█████████▍| 30/32 [02:42<00:10,  5.47s/it]Mix quant:  57%|█████▊    | 23/40 [02:37<02:16,  8.02s/it]Mix quant:  97%|█████████▋| 31/32 [02:47<00:05,  5.70s/it]Mix quant:  97%|█████████▋| 31/32 [02:48<00:05,  5.71s/it]Mix quant:  60%|██████    | 24/40 [02:44<02:10,  8.15s/it]Mix quant:  60%|██████    | 24/40 [02:46<02:10,  8.15s/it]Mix quant: 100%|██████████| 32/32 [02:52<00:00,  5.29s/it]Mix quant: 100%|██████████| 32/32 [02:52<00:00,  5.38s/it]
Mix quant: 100%|██████████| 32/32 [02:52<00:00,  5.28s/it]Mix quant: 100%|██████████| 32/32 [02:52<00:00,  5.40s/it]
Mix quant:  62%|██████▎   | 25/40 [02:51<01:56,  7.74s/it]Mix quant:  62%|██████▎   | 25/40 [02:52<01:55,  7.73s/it]Mix Model is quantized and saved at "/home/dataset/quant/quant4/down_weight_only/Llama-2-7b"
Mix Model is quantized and saved at "/home/dataset/quant/quant8/down_weight_only/Llama-2-7b"
Mix quant:  65%|██████▌   | 26/40 [04:40<08:50, 37.91s/it]Mix quant:  65%|██████▌   | 26/40 [04:41<08:50, 37.92s/it]Mix quant:  68%|██████▊   | 27/40 [04:45<06:07, 28.30s/it]Mix quant:  68%|██████▊   | 27/40 [04:47<06:07, 28.29s/it]Mix quant:  70%|███████   | 28/40 [04:52<04:20, 21.72s/it]Mix quant:  70%|███████   | 28/40 [04:53<04:20, 21.69s/it]Mix quant:  72%|███████▎  | 29/40 [04:56<03:00, 16.41s/it]Mix quant:  72%|███████▎  | 29/40 [04:57<03:00, 16.40s/it]Mix quant:  75%|███████▌  | 30/40 [05:02<02:12, 13.22s/it]Mix quant:  75%|███████▌  | 30/40 [05:03<02:12, 13.22s/it]Mix quant:  78%|███████▊  | 31/40 [05:08<01:39, 11.07s/it]Mix quant:  78%|███████▊  | 31/40 [05:09<01:39, 11.06s/it]Mix quant:  80%|████████  | 32/40 [05:13<01:15,  9.45s/it]Mix quant:  80%|████████  | 32/40 [05:14<01:15,  9.46s/it]Mix quant:  82%|████████▎ | 33/40 [05:19<00:59,  8.45s/it]Mix quant:  82%|████████▎ | 33/40 [05:21<00:59,  8.45s/it]Mix quant:  85%|████████▌ | 34/40 [05:26<00:47,  7.88s/it]Mix quant:  85%|████████▌ | 34/40 [05:27<00:47,  7.87s/it]Mix quant:  88%|████████▊ | 35/40 [05:32<00:36,  7.34s/it]Mix quant:  88%|████████▊ | 35/40 [05:33<00:36,  7.34s/it]Mix quant:  90%|█████████ | 36/40 [05:38<00:27,  6.85s/it]Mix quant:  90%|█████████ | 36/40 [05:39<00:27,  6.88s/it]Mix quant:  92%|█████████▎| 37/40 [05:43<00:19,  6.51s/it]Mix quant:  92%|█████████▎| 37/40 [05:45<00:19,  6.52s/it]Mix quant:  95%|█████████▌| 38/40 [05:48<00:12,  6.03s/it]Mix quant:  95%|█████████▌| 38/40 [05:50<00:12,  6.03s/it]Mix quant:  98%|█████████▊| 39/40 [05:53<00:05,  5.70s/it]Mix quant:  98%|█████████▊| 39/40 [05:55<00:05,  5.70s/it]Mix quant: 100%|██████████| 40/40 [05:57<00:00,  5.19s/it]Mix quant: 100%|██████████| 40/40 [05:57<00:00,  8.94s/it]
do not have the file  /home/dataset/quant/quant8/down_weight_only/Llama-2-13b/pytorch_model.bin
Mix quant: 100%|██████████| 40/40 [05:58<00:00,  5.18s/it]Mix quant: 100%|██████████| 40/40 [05:58<00:00,  8.97s/it]
Mix Model is quantized and saved at "/home/dataset/quant/quant8/down_weight_only/Llama-2-13b"
Mix Model is quantized and saved at "/home/dataset/quant/quant4/down_weight_only/Llama-2-13b"
srun: Requested partition configuration not available now
srun: job 172183 queued and waiting for resources
srun: Requested partition configuration not available now
srun: job 172184 queued and waiting for resources
srun: Requested partition configuration not available now
srun: job 172185 queued and waiting for resources
srun: Requested partition configuration not available now
srun: job 172186 queued and waiting for resources
srun: Job has been cancelled
srun: Job has been cancelled
srun: Job has been cancelled
srun: Job has been cancelled
srun: error: Unable to allocate resources: Job/step already completing or completed
srun: error: Unable to allocate resources: Job/step already completing or completed
srun: error: Unable to allocate resources: Job/step already completing or completed
srun: error: Unable to allocate resources: Job/step already completing or completed
